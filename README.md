# ğŸš¤ KABOAT-2026

> **A ROS 2â€“based autonomous navigation and control framework for an Unmanned Surface Vehicle (USV), developed for the KABOAT 2026 competition.**

---

## ğŸ›  System Configuration

### ğŸ” Sensors
- **LiDAR** â€“ Obstacle detection and environment mapping  
- **IMU (Inertial Measurement Unit)** â€“ Attitude and heading estimation  
- **GPS (Global Positioning System)** â€“ Global positioning and waypoint navigation  
- **Vision Camera** â€“ Object detection and visual perception  

### âš™ï¸ Actuators
- **Servo Motor** â€“ Steering control  
- **Thruster** â€“ Propulsion control  

---

## ğŸ§­ Coordinate Frame Definition

To ensure consistent spatial representation across heterogeneous sensors, a unified coordinate framework is established.

### ğŸ”„ Coordinate Frame Alignment & Fusion
- **LiDAR data** are represented in the vessel-relative coordinate frame.  
- **GPS and IMU data** are initially expressed in the global (absolute) reference frame.  
- Global measurements are transformed into a **vessel-fixed coordinate system**, aligned with the **bow-referenced X-axis**.  
- All sensor data are integrated into this unified frame, enabling robust spatial consistency and reliable sensor fusion.

# ğŸ Competition Field Overview

## ğŸŒŠ Overall Field Layout
- Description of the complete competition environment  
- Includes waypoint zones, buoy obstacles, hopping section, and docking area  

<img width="712" height="336" alt="image" src="https://github.com/user-attachments/assets/5e95a6dc-3ed7-46eb-a2af-65c2fd03a18e" />

---

# ğŸ“ Course-Specific Algorithms

---

# ğŸ¥‡ Course 1 â€“ GPS-Based Navigation with LiDAR Obstacle Avoidance

## ğŸ“¡ Sensor Initialization
All sensors (LiDAR, IMU, GPS, Camera) are activated and continuously subscribed via ROS 2 callbacks from system startup.

At every control frame, the **servo target angle** is determined according to the following priority:

---

## ğŸ¯ Steering Decision Priority

### 1ï¸âƒ£ Primary: GPS Heading Tracking
The vessel follows the heading angle toward the target GPS waypoint:
- First waypoint  
- After passing all buoy obstacles  
- Entry point before the purple buoy (hopping section)

---

### 2ï¸âƒ£ Secondary: LiDAR-Based Obstacle Avoidance

If LiDAR detects an obstacle within a predefined safety distance:

1. Select front **180Â° LiDAR scan data**
2. Divide into **180 angular bins (1Â° resolution)**
3. Average values inside each bin â†’ produce a 180-element distance array (unit: meters)

Each distance value is mapped to a risk score using an exponential function

- Closer distance â†’ Higher risk (0â€“100 scale)
- Risk above threshold â†’ Marked as **Danger (0)**
- Otherwise â†’ **Safe (1)**

### ğŸš§ Vessel Width Compensation
For every detected danger index:
- Extend danger marking to Â±5 neighboring indices
- Ensures collision-free clearance considering vessel width

### âœ… Final Steering Selection
Among all remaining safe angles:
- Select the angle closest to the GPS target heading.

---

## ğŸ”„ Transition to Hopping Mode

Upon reaching the first waypoint:

- IMU heading is adjusted to approximately **+45Â° starboard turn**
- Purpose: Bring purple buoy into cameraâ€™s left field of view
- If buoy detected **or 3 seconds elapsed**, transition to Course 2

---

# ğŸ¥ˆ Course 2 â€“ Vision-LiDAR Fusion & Hopping Algorithm

## ğŸ¥ Sensor Calibration

- Camera faces **-90Â° (port side)**
- LiDAR scans **front 180Â°**
- Overlapping usable range: **0Â° to -90Â°**

Pixel offset from camera center is converted into angular displacement.
This is aligned with LiDAR angular measurements.

---

## ğŸ” Hopping Control Logic (Line-Tracing Principle)

1. Follow IMU heading â‰ˆ **135Â°**
2. When buoy detected:
   - Adjust steering so buoy center aligns with **-90Â° vessel heading**
3. If buoy center = -90Â° â†’ Move straight
4. If buoy center = -110Â° (example threshold):
   - Command 30Â° port turn (thruster 60%)

Since vessel circles buoy from right side:
- Exit loop after IMU heading crosses 0Â° twice

---

## ğŸ¯ Target Object Recognition

After loop exit:

- Follow IMU -90Â°
- Move straight until camera detects target object
- Align object center with camera center (-90Â° vessel heading)
- Activate LED corresponding to object color
- Continue to second waypoint (junction center)

---

# ğŸ¥‰ Course 3 â€“ LiDAR-Based Free-Space Navigation (Docking)

Camera is disabled.
Navigation relies on **LiDAR + IMU only**.

Unlike Course 1:
- No GPS heading tracking
- Follow the direction of **maximum LiDAR distance**

---

## ğŸ“ Steering Decision Logic

At each LiDAR callback frame:

1. Compute intersection between:
   - IMU heading range (0Â° to -180Â°)
   - LiDAR front 180Â° data

2. Identify angle with **maximum distance**

3. If maximum distance â‰¤ 0.3 m:
   - Stop immediately

---

## ğŸ“Š LiDAR Processing

- Front 180Â° divided into 180 bins
- Averaged per 1Â°
- Produces 180-element distance list

Selecting the maximum-distance angle naturally:
- Guides vessel toward open space
- Avoids walls
- Handles corner navigation implicitly

Turning radius is controlled via thruster output modulation.

The stopping threshold (0.3 m) remains active throughout Course 3.

---

## ğŸ”§ Algorithm Extensibility

Although the farthest-distance strategy is the simplest implementation, the following can be incorporated:

- Course 1â€“style obstacle risk evaluation
- Adaptive safety angle generation
- Turning radiusâ€“aware safe region selection

---

